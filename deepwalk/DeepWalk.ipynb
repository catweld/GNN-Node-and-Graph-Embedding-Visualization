{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DeepWalk.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1XYiaqwmC-3KXcbdWV-XLWgrej1U_sri8","authorship_tag":"ABX9TyMhBAhK7itdIfATfWWYW2p5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"_1mL7PTCAlGp","executionInfo":{"status":"ok","timestamp":1623722156387,"user_tz":240,"elapsed":193,"user":{"displayName":"Catherine Guimaraes Weldon","photoUrl":"","userId":"06262169761686508553"}}},"source":["#!pip install dgl\n","import sys\n","sys.path.append('/content/drive/MyDrive/Thesis/deepwalk/')\n","import torch\n","import argparse\n","import dgl\n","import torch.multiprocessing as mp\n","from torch.utils.data import DataLoader\n","import os\n","import random\n","import time\n","import numpy as np\n","\n","from reading_data import DeepwalkDataset\n","from model import SkipGramModel\n","from utils import thread_wrapped_func, shuffle_walks, sum_up_params\n","from dgl.data import register_data_args, load_data\n","\n","class DeepwalkTrainer:\n","    def __init__(self, args):\n","        \"\"\" Initializing the trainer with the input arguments \"\"\"\n","        self.args = args\n","        # self.dataset = DeepwalkDataset(\n","        #     net_file=args.data_file,\n","        #     map_file=args.map_file,\n","        #     walk_length=args.walk_length,\n","        #     window_size=args.window_size,\n","        #     num_walks=args.num_walks,\n","        #     batch_size=args.batch_size,\n","        #     negative=args.negative,\n","        #     gpus=args.gpus,\n","        #     fast_neg=args.fast_neg,\n","        #     ogbl_name=args.ogbl_name,\n","        #     load_from_ogbl=args.load_from_ogbl,\n","        #     )\n","        self.dataset = load_data(args)\n","        self.emb_size = self.dataset.graph.number_of_nodes()\n","        self.emb_model = None\n","\n","    def init_device_emb(self):\n","        \"\"\" set the device before training \n","        will be called once in fast_train_mp / fast_train\n","        \"\"\"\n","        choices = sum([self.args.only_gpu, self.args.only_cpu, self.args.mix])\n","        assert choices == 1, \"Must choose only *one* training mode in [only_cpu, only_gpu, mix]\"\n","        \n","        # initializing embedding on CPU\n","        self.emb_model = SkipGramModel(\n","            emb_size=self.emb_size, \n","            emb_dimension=self.args.dim,\n","            walk_length=self.args.walk_length,\n","            window_size=self.args.window_size,\n","            batch_size=self.args.batch_size,\n","            only_cpu=self.args.only_cpu,\n","            only_gpu=self.args.only_gpu,\n","            mix=self.args.mix,\n","            neg_weight=self.args.neg_weight,\n","            negative=self.args.negative,\n","            lr=self.args.lr,\n","            lap_norm=self.args.lap_norm,\n","            fast_neg=self.args.fast_neg,\n","            record_loss=self.args.print_loss,\n","            norm=self.args.norm,\n","            use_context_weight=self.args.use_context_weight,\n","            async_update=self.args.async_update,\n","            num_threads=self.args.num_threads,\n","            )\n","        \n","        torch.set_num_threads(self.args.num_threads)\n","        if self.args.only_gpu:\n","            print(\"Run in 1 GPU\")\n","            assert self.args.gpus[0] >= 0\n","            self.emb_model.all_to_device(self.args.gpus[0])\n","        elif self.args.mix:\n","            print(\"Mix CPU with %d GPU\" % len(self.args.gpus))\n","            if len(self.args.gpus) == 1:\n","                assert self.args.gpus[0] >= 0, 'mix CPU with GPU should have available GPU'\n","                self.emb_model.set_device(self.args.gpus[0])\n","        else:\n","            print(\"Run in CPU process\")\n","            self.args.gpus = [torch.device('cpu')]\n","\n","\n","    def train(self):\n","        \"\"\" train the embedding \"\"\"\n","        if len(self.args.gpus) > 1:\n","            self.fast_train_mp()\n","        else:\n","            self.fast_train()\n","\n","    def fast_train_mp(self):\n","        \"\"\" multi-cpu-core or mix cpu & multi-gpu \"\"\"\n","        self.init_device_emb()\n","        self.emb_model.share_memory()\n","\n","        if self.args.count_params:\n","            sum_up_params(self.emb_model)\n","\n","        start_all = time.time()\n","        ps = []\n","\n","        for i in range(len(self.args.gpus)):\n","            p = mp.Process(target=self.fast_train_sp, args=(i, self.args.gpus[i]))\n","            ps.append(p)\n","            p.start()\n","\n","        for p in ps:\n","            p.join()\n","        \n","        print(\"Used time: %.2fs\" % (time.time()-start_all))\n","        if self.args.save_in_txt:\n","            self.emb_model.save_embedding_txt(self.dataset, self.args.output_emb_file)\n","        elif self.args.save_in_pt:\n","            self.emb_model.save_embedding_pt(self.dataset, self.args.output_emb_file)\n","        else:\n","            self.emb_model.save_embedding(self.dataset, self.args.output_emb_file)\n","\n","    @thread_wrapped_func\n","    def fast_train_sp(self, rank, gpu_id):\n","        \"\"\" a subprocess for fast_train_mp \"\"\"\n","        if self.args.mix:\n","            self.emb_model.set_device(gpu_id)\n","        \n","        torch.set_num_threads(self.args.num_threads)\n","        if self.args.async_update:\n","            self.emb_model.create_async_update()\n","\n","        sampler = self.dataset.create_sampler(rank)\n","\n","        dataloader = DataLoader(\n","            dataset=sampler.seeds,\n","            batch_size=self.args.batch_size,\n","            collate_fn=sampler.sample,\n","            shuffle=False,\n","            drop_last=False,\n","            num_workers=self.args.num_sampler_threads,\n","            )\n","        num_batches = len(dataloader)\n","        print(\"num batchs: %d in process [%d] GPU [%d]\" % (num_batches, rank, gpu_id))\n","        # number of positive node pairs in a sequence\n","        num_pos = int(2 * self.args.walk_length * self.args.window_size\\\n","            - self.args.window_size * (self.args.window_size + 1))\n","        \n","        start = time.time()\n","        with torch.no_grad():\n","            for i, walks in enumerate(dataloader):\n","                if self.args.fast_neg:\n","                    self.emb_model.fast_learn(walks)\n","                else:\n","                    # do negative sampling\n","                    bs = len(walks)\n","                    neg_nodes = torch.LongTensor(\n","                        np.random.choice(self.dataset.neg_table, \n","                            bs * num_pos * self.args.negative, \n","                            replace=True))\n","                    self.emb_model.fast_learn(walks, neg_nodes=neg_nodes)\n","\n","                if i > 0 and i % self.args.print_interval == 0:\n","                    if self.args.print_loss:\n","                        print(\"GPU-[%d] batch %d time: %.2fs loss: %.4f\" \\\n","                            % (gpu_id, i, time.time()-start, -sum(self.emb_model.loss)/self.args.print_interval))\n","                        self.emb_model.loss = []\n","                    else:\n","                        print(\"GPU-[%d] batch %d time: %.2fs\" % (gpu_id, i, time.time()-start))\n","                    start = time.time()\n","\n","            if self.args.async_update:\n","                self.emb_model.finish_async_update()\n","\n","    def fast_train(self):\n","        \"\"\" fast train with dataloader with only gpu / only cpu\"\"\"\n","        # the number of postive node pairs of a node sequence\n","        num_pos = 2 * self.args.walk_length * self.args.window_size\\\n","            - self.args.window_size * (self.args.window_size + 1)\n","        num_pos = int(num_pos)\n","\n","        self.init_device_emb()\n","\n","        if self.args.async_update:\n","            self.emb_model.share_memory()\n","            self.emb_model.create_async_update()\n","\n","        if self.args.count_params:\n","            sum_up_params(self.emb_model)\n","\n","        #sampler = self.dataset.create_sampler(0)\n","\n","        dataloader = DataLoader(\n","            dataset=torch.tensor(self.dataset),\n","            batch_size=self.args.batch_size,\n","            shuffle=False,\n","            drop_last=False,\n","            num_workers=self.args.num_sampler_threads,\n","            )\n","        \n","        num_batches = len(dataloader)\n","        print(\"num batchs: %d\\n\" % num_batches)\n","\n","        start_all = time.time()\n","        start = time.time()\n","        with torch.no_grad():\n","            max_i = num_batches\n","            for i, walks in enumerate(dataloader):\n","                if self.args.fast_neg:\n","                    self.emb_model.fast_learn(walks)\n","                else:\n","                    # do negative sampling\n","                    bs = len(walks)\n","                    neg_nodes = torch.LongTensor(\n","                        np.random.choice(self.dataset.neg_table, \n","                            bs * num_pos * self.args.negative, \n","                            replace=True))\n","                    self.emb_model.fast_learn(walks, neg_nodes=neg_nodes)\n","\n","                if i > 0 and i % self.args.print_interval == 0:\n","                    if self.args.print_loss:\n","                        print(\"Batch %d training time: %.2fs loss: %.4f\" \\\n","                            % (i, time.time()-start, -sum(self.emb_model.loss)/self.args.print_interval))\n","                        self.emb_model.loss = []\n","                    else:\n","                        print(\"Batch %d, training time: %.2fs\" % (i, time.time()-start))\n","                    start = time.time()\n","\n","            if self.args.async_update:\n","                self.emb_model.finish_async_update()\n","\n","        print(\"Training used time: %.2fs\" % (time.time()-start_all))\n","        if self.args.save_in_txt:\n","            self.emb_model.save_embedding_txt(self.dataset, self.args.output_emb_file)\n","        elif self.args.save_in_pt:\n","            self.emb_model.save_embedding_pt(self.dataset, self.args.output_emb_file)\n","        else:\n","            self.emb_model.save_embedding(self.dataset, self.args.output_emb_file)\n","\n","if __name__ == '__main__':\n","    parser = argparse.ArgumentParser(description=\"DeepWalk\")\n","    # input files\n","    ## personal datasets\n","    parser.add_argument(\"-f\", \"--fff\", help=\"a dummy argument to fool ipython\", default=\"1\")\n","    parser.add_argument('--data_file', type=str, \n","            help=\"path of the txt network file, builtin dataset include youtube-net and blog-net\") \n","    ## ogbl datasets\n","    parser.add_argument('--ogbl_name', type=str, \n","            help=\"name of ogbl dataset, e.g. ogbl-ddi\")\n","    parser.add_argument('--load_from_ogbl', default=False, action=\"store_true\",\n","            help=\"whether load dataset from ogbl\")\n","\n","    # output files\n","    parser.add_argument('--save_in_txt', default=False, action=\"store_true\",\n","            help='Whether save dat in txt format or npy')\n","    parser.add_argument('--save_in_pt', default=False, action=\"store_true\",\n","            help='Whether save dat in pt format or npy')\n","    parser.add_argument('--output_emb_file', type=str, default=\"emb.npy\",\n","            help='path of the output npy embedding file')\n","    parser.add_argument('--map_file', type=str, default=\"nodeid_to_index.pickle\",\n","            help='path of the mapping dict that maps node ids to embedding index')\n","    parser.add_argument('--norm', default=False, action=\"store_true\", \n","            help=\"whether to do normalization over node embedding after training\")\n","    \n","    # model parameters\n","    parser.add_argument('--dim', default=128, type=int, \n","            help=\"embedding dimensions\")\n","    parser.add_argument('--window_size', default=5, type=int, \n","            help=\"context window size\")\n","    parser.add_argument('--use_context_weight', default=False, action=\"store_true\", \n","            help=\"whether to add weights over nodes in the context window\")\n","    parser.add_argument('--num_walks', default=10, type=int, \n","            help=\"number of walks for each node\")\n","    parser.add_argument('--negative', default=1, type=int, \n","            help=\"negative samples for each positve node pair\")\n","    parser.add_argument('--batch_size', default=128, type=int, \n","            help=\"number of node sequences in each batch\")\n","    parser.add_argument('--walk_length', default=80, type=int, \n","            help=\"number of nodes in a sequence\")\n","    parser.add_argument('--neg_weight', default=1., type=float, \n","            help=\"negative weight\")\n","    parser.add_argument('--lap_norm', default=0.01, type=float, \n","            help=\"weight of laplacian normalization, recommend to set as 0.1 / windoe_size\")\n","    \n","    # training parameters\n","    parser.add_argument('--print_interval', default=100, type=int, \n","            help=\"number of batches between printing\")\n","    parser.add_argument('--print_loss', default=False, action=\"store_true\", \n","            help=\"whether print loss during training\")\n","    parser.add_argument('--lr', default=0.2, type=float, \n","            help=\"learning rate\")\n","    \n","    # optimization settings\n","    parser.add_argument('--mix', default=False, action=\"store_true\", \n","            help=\"mixed training with CPU and GPU\")\n","    parser.add_argument('--gpus', type=int, default=[-1], nargs='+', \n","            help='a list of active gpu ids, e.g. 0, used with --mix')\n","    parser.add_argument('--only_cpu', default=True, action=\"store_true\", \n","            help=\"training with CPU\")\n","    parser.add_argument('--only_gpu', default=False, action=\"store_true\", \n","            help=\"training with GPU\")\n","    parser.add_argument('--async_update', default=False, action=\"store_true\", \n","            help=\"mixed training asynchronously, not recommended\")\n","\n","    parser.add_argument('--fast_neg', default=False, action=\"store_true\", \n","            help=\"do negative sampling inside a batch\")\n","    parser.add_argument('--num_threads', default=8, type=int, \n","            help=\"number of threads used for each CPU-core/GPU\")\n","    parser.add_argument('--num_sampler_threads', default=0, type=int, \n","            help=\"number of threads used for sampling\")\n","    \n","    parser.add_argument('--count_params', default=False, action=\"store_true\", \n","            help=\"count the params, exit once counting over\")\n","\n","    args = parser.parse_args()\n","\n","    if args.async_update:\n","        assert args.mix, \"--async_update only with --mix\"\n","\n","    start_time = time.time()\n","    args.dataset = 'cora'\n","    trainer = DeepwalkTrainer(args)\n","    trainer.train()\n","    print(\"Total used time: %.2f\" % (time.time() - start_time))"],"execution_count":1,"outputs":[]}]}